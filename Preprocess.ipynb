{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-03-17T03:31:45.604194Z",
     "start_time": "2025-03-17T03:31:43.965149Z"
    }
   },
   "source": [
    "import lzma\n",
    "import json\n",
    "import pandas as pd\n",
    "from geopy.geocoders import GoogleV3\n",
    "import os\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import langid\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import nltk"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Job Description",
   "id": "6264986df18242f6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T03:31:59.402777Z",
     "start_time": "2025-03-17T03:31:59.396406Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# load job ad data in .json.xz format\n",
    "job_ad_file_path = 'data/ads-50k.json.xz'\n",
    "\n",
    "job_ad_array = []\n",
    "# Read the compressed JSON file into a DataFrame\n",
    "with lzma.open(job_ad_file_path, 'rt', encoding='utf-8') as file:\n",
    "    for i, line in enumerate(file):\n",
    "        data = json.loads(line.strip())\n",
    "        job_ad_array.append(data)\n",
    "if job_ad_array:\n",
    "    job_ad_df = pd.DataFrame(job_ad_array)\n",
    "\n"
   ],
   "id": "2a8b3b9786067282",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Convert location info to coordinate",
   "id": "7fdd5779c9d1731a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T03:32:06.666744Z",
     "start_time": "2025-03-17T03:32:04.320399Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def combine_location_area(row:object)->str:\n",
    "    '''\n",
    "    Aggregate values from three location related columns(location, suburb, area)\n",
    "    After analysis, there is not missing data from location, 17156 missing from area and 12998 from suburb. The value of suburb is the most appropriate level of data, but due to missing data issue, and possible duplicated suburb name across Australia, if suburb data is available, we use location+suburb, if suburb data is unavailable but area data is available, we use location+area instead, or we use location only if both suburb and area are unavailable.\n",
    "    :param row:\n",
    "    :return:\n",
    "    '''\n",
    "    # if a suburb is not found and can't be found in location\n",
    "    if pd.notnull(row['suburb']):\n",
    "        if row['suburb'] == row['location']:\n",
    "            return row['location']\n",
    "        elif row['suburb'] not in row['location']:\n",
    "            combined = f\"{row['location']} {row['suburb']}\"\n",
    "            return combined\n",
    "    # if suburb cant be found, use area info to enrich location info\n",
    "    elif pd.notnull(row['area']) and row['area'] not in row['location']:  # Check if area is not null\n",
    "        # Split area into components using regex\n",
    "        # areas = [item.strip() for item in re.split(r', | & ', row['area'])]\n",
    "        # Combine each area component with location\n",
    "        combined = f\"{row['location']} {row['area']}\"\n",
    "        return combined\n",
    "    return row['location']  # Return location as a list if area is null\n",
    "\n",
    "def getLocationInfo(location_name:str)->dict:\n",
    "    '''\n",
    "    Call google API to get location details based on address provided\n",
    "    :param location_name: address\n",
    "    :return: location_name,location.address, location.latitude, location.longitude\n",
    "    '''\n",
    "    # Initialize GoogleV3 API with your API key\n",
    "    api_key = \"AIzaSyCe9mKkMM6-zHF060DakgrULDAWWvUmtDA\"\n",
    "    geolocator = GoogleV3(api_key=api_key)\n",
    "    # Provide the location name\n",
    "    # location_name = \"Shepparton & Goulburn Valley\"\n",
    "\n",
    "    # Get location information\n",
    "    location = geolocator.geocode(location_name)\n",
    "\n",
    "    # Extract latitude and longitude\n",
    "    if location:\n",
    "        return location_name,location.address, location.latitude, location.longitude\n",
    "    else:\n",
    "        print(\"Location not found.\")\n",
    "        return location_name,None,None,None\n",
    "def check_oversea(row:object)->object:\n",
    "    '''\n",
    "    CHECK THE COUNTRY OF AN ADDRESS\n",
    "    :param value: the address value from the dataframe\n",
    "    :return:\n",
    "    if the address is an Australian address, return 0\n",
    "    else if the address is a New Zealand address, return 1\n",
    "    else return 3 for rest of countries\n",
    "    '''\n",
    "    if 'Australia' in row[\"address\"]:\n",
    "        row[\"global\"] = 0\n",
    "        row[\"country\"] = 'Australia'\n",
    "    elif 'New Zealand' in row[\"address\"]:\n",
    "        row[\"global\"] = 1\n",
    "        row[\"country\"] = 'New Zealand'\n",
    "    else:\n",
    "        row[\"global\"] = 3\n",
    "        row[\"country\"] = row[\"address\"].split(',')[-1]\n",
    "    return row\n",
    "\n",
    "def update_valid_state_data(row:object)->object:\n",
    "    '''\n",
    "    Get state data from address for Australian addresses\n",
    "    :param row: row data\n",
    "    :return: row data\n",
    "    '''\n",
    "    valid_state_list = ['VIC','QLD','NSW','SA','NT','TAS','ACT']\n",
    "    if row['global']==0:\n",
    "        for state in valid_state_list:\n",
    "            if state in row['address']:\n",
    "                row['state'] = state\n",
    "                return row\n",
    "    else:\n",
    "        if 'Victoria' in row['address']:\n",
    "            row['state'] = 'VIC'\n",
    "            return row\n",
    "        elif 'Queensland' in row['address']:\n",
    "            row['state'] =  'QLD'\n",
    "            return row\n",
    "        elif 'Tasmania' in row['address']:\n",
    "            row['state'] =  'TAS'\n",
    "            return row\n",
    "        elif 'Canberra' in row['address']:\n",
    "            row['state'] =  'ACT'\n",
    "            return row\n",
    "    row['state'] = 'NA'\n",
    "    return row"
   ],
   "id": "da78279c4e085fb9",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T06:54:11.298450Z",
     "start_time": "2025-03-17T06:54:09.338012Z"
    }
   },
   "cell_type": "code",
   "source": [
    "expanded_df = pd.json_normalize(job_ad_df['metadata'])\n",
    "# Combine the expanded columns with the original DataFrame\n",
    "job_ad_extend_df = pd.concat([job_ad_df.drop(columns=['metadata']), expanded_df], axis=1)\n",
    "# Rename columns using a dictionary\n",
    "job_ad_extend_df = job_ad_extend_df.rename(columns={\n",
    "    'classification.name': 'classification',\n",
    "    'subClassification.name': 'subClassification',\n",
    "    'location.name': 'location',\n",
    "    'workType.name': 'workType',\n",
    "    'area.name': 'area',\n",
    "    'suburb.name': 'suburb'\n",
    "})"
   ],
   "id": "506a69edf4806301",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T03:32:11.928782Z",
     "start_time": "2025-03-17T03:32:11.562836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# generate a new column by combining location and area\n",
    "job_ad_extend_df['area_location'] = job_ad_extend_df.apply(combine_location_area, axis=1)\n",
    "# drop unnecessary columns\n",
    "job_ad_extend_df = job_ad_extend_df.drop(['location', 'area', 'suburb'], axis=1)"
   ],
   "id": "796f01b7e7ec9c86",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T03:32:14.635496Z",
     "start_time": "2025-03-17T03:32:14.610271Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# a temp file to avoid call api all the time\n",
    "file_path = \"data/coordinate.csv\"\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_path):\n",
    "    print(f\"The file '{file_path}' exists.\")\n",
    "    coordinate_data_df = pd.read_csv(file_path,index_col=0)\n",
    "else:\n",
    "    coordinate_data = [getLocationInfo(loc) for loc in unique_area_location]\n",
    "    incomplete_coordinate_data = [item[0] for item in coordinate_data if item[1] is None]\n",
    "    # remove if location can not be found\n",
    "    coordinate_data = [item for item in coordinate_data if item[1] is not None]\n",
    "    # 1st fixing for ACT recognition issue\n",
    "    fixing_ACT = [item.replace('ACT','Canberra') for item in incomplete_coordinate_data if 'ACT' in item]\n",
    "    coordinate_data_ACT = [getLocationInfo(loc) for loc in fixing_ACT]\n",
    "    coordinate_data_ACT = [[item[0].replace('Canberra','ACT'),item[1],item[2],item[3]] for item in coordinate_data_ACT if item[1] is not None]\n",
    "    # there are still some locations cant be recognized by google API,\n",
    "    fixed_ACT_list = [item[0] for item in coordinate_data_ACT]\n",
    "    incomplete_coordinate_data = [item for item in incomplete_coordinate_data if item not in fixed_ACT_list]\n",
    "    len(job_ad_extend_df[job_ad_extend_df[\"area_location\"].isin(incomplete_coordinate_data)])\n",
    "    job_ad_extend_filtered_df = job_ad_extend_df[~job_ad_extend_df['area_location'].isin(incomplete_coordinate_data)]\n",
    "    coordinate_data_df = pd.DataFrame(coordinate_data, columns=['mapping_address','address', 'Latitude', 'Longitude'])\n",
    "    coordinate_data_df = coordinate_data_df.apply(check_oversea, axis=1)\n",
    "    coordinate_data_df = coordinate_data_df.apply(update_valid_state_data, axis=1)\n",
    "    coordinate_data_df.to_csv('data/coordinate.csv', index=False)"
   ],
   "id": "fe8bfabcdc8e8a88",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The file 'data/coordinate.csv' exists.\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T03:32:25.040001Z",
     "start_time": "2025-03-17T03:32:24.946016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# join coordinates with job ad dataframe\n",
    "job_ad_extend_coord_df = job_ad_extend_df.merge(coordinate_data_df, left_on='area_location', right_on='mapping_address', how='left')[['id', 'title', 'abstract', 'content', 'additionalSalaryText',\n",
    "       'classification', 'subClassification', 'workType', 'area_location',\n",
    "        'address', 'Latitude', 'Longitude',\n",
    "       'global', 'state', 'country']]\n",
    "# drop data with incomplete address(326 rows affected)\n",
    "job_ad_extend_coord_df = job_ad_extend_coord_df[job_ad_extend_coord_df[\"address\"].notnull()]"
   ],
   "id": "8ae6f66689117ae0",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Clean text data",
   "id": "6ce3b09dcbefb916"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T03:38:46.591663Z",
     "start_time": "2025-03-17T03:35:15.212509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "custom_stop_words = [\"please\",\"letter\",\"within\",\"at\",\"of\",\"looking\",\"their\",\"they\",\"about\",\"are\",\"if\",\"have\",\"is\",\"us\",\"on\",\"our\",\"this\",\"to\",\"be\",\"for\",\"in\",\"with\",\"i\",\"you\",\"we\",\"a\",\"and\",\"the\",\"will\", \"join\", \"work\", \"opportunity\", \"new\", \"role\", \"based\",\"company\",\"currently\",\"duty\",\"candidate\",\"applicant\",\"end\",\"position\",\"skills\",\"ability\",'application','applications',\"apply\",\"click\",\"responsibility\",\"responsibilities\"]\n",
    "# Function to extract and clean text from HTML\n",
    "def extract_text_from_html(html:str)->str:\n",
    "    '''\n",
    "    Remove html related syntax from content\n",
    "    :param html: text data\n",
    "    :return: cleaned text\n",
    "    '''\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    text = soup.get_text(separator=' ')  # Extract text and separate paragraphs with spaces\n",
    "    return text\n",
    "def preprocess_text(text:str)->str:\n",
    "    '''\n",
    "    Preprocess text data\n",
    "    :param text: jd text data\n",
    "    :return: cleaned text\n",
    "    '''\n",
    "    # 1) Lowercase\n",
    "    text = text.lower()\n",
    "    # 2) Remove punctuation (optional)\n",
    "    text = ''.join([char for char in text if char.isalnum() or char == ' '])\n",
    "    # 3) Remove words with numbers\n",
    "    text = re.sub(r'\\b\\w*\\d+\\w*\\b', '', text)  # Removes words with numbers\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = text.split()\n",
    "    # 4) Remove stopwords\n",
    "    filtered_words = [word for word in words if (word not in stop_words) and (word not in custom_stop_words)]\n",
    "    # 5) Apply lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "    return ' '.join(lemmatized_words)\n",
    "\n",
    "def is_english(text:str)->bool:\n",
    "    '''\n",
    "    Check if text is in English language\n",
    "    :param text: input text\n",
    "    :return: True if text is in English language\n",
    "    '''\n",
    "    predictions = langid.classify(text)  # Predict the top language\n",
    "    return predictions[0] == 'en' or predictions[0] == 'fr'\n",
    "\n",
    "# Apply HTML text extraction\n",
    "job_ad_extend_coord_df['cleaned_content'] = job_ad_extend_coord_df['content'].apply(extract_text_from_html)\n",
    "# Combine abstract and content for analysis\n",
    "job_ad_extend_coord_df['abstract_content'] = job_ad_extend_coord_df['abstract'] + \" \" + job_ad_extend_coord_df['cleaned_content']\n",
    "job_ad_extend_coord_df['abstract_content'] = job_ad_extend_coord_df['abstract_content'].apply(preprocess_text)\n",
    "job_ad_extend_coord_df['cleaned_title'] = job_ad_extend_coord_df['title'].apply(preprocess_text)\n",
    "job_ad_extend_coord_df[\"is_english\"] = job_ad_extend_coord_df['abstract_content'].apply(is_english)\n",
    "#remove non-english records\n",
    "job_ad_extend_coord_df = job_ad_extend_coord_df[job_ad_extend_coord_df[\"is_english\"]]"
   ],
   "id": "767fe4ad566477d3",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Exract Salary Info",
   "id": "74426e8a9f0197db"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T03:38:49.425410Z",
     "start_time": "2025-03-17T03:38:46.633645Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_salary_info(text:str)->dict:\n",
    "    '''\n",
    "    Extract salary information from text\n",
    "    :param text: salary text\n",
    "    :return: salary information in (salary_value, salary_unit) format\n",
    "    '''\n",
    "    # Helper function to clean and convert numbers\n",
    "    def parse_value(value_str, is_k=False):\n",
    "        value = float(value_str.replace(',', '')) if value_str else 0\n",
    "        return value * 1000 if is_k else value\n",
    "    if type(text).__name__ == \"str\":\n",
    "        # Regex patterns with priority order\n",
    "        patterns = [\n",
    "            # Hourly rates\n",
    "            (r'(?:(\\$|£|€)?(\\d[\\d,.]*)(k?)\\s*(?:to|-|up\\s*to)\\s*)?(\\$|£|€)?(\\d[\\d,.]*)(k?)\\s*(?:p\\.h\\.|per\\s+hour|/hr?)\\b',\n",
    "             lambda m: ((parse_value(m.group(2), bool(m.group(3))) + parse_value(m.group(5), bool(m.group(6))))/2)\n",
    "                       if m.group(2) else parse_value(m.group(5), bool(m.group(6))), 'hour'),\n",
    "\n",
    "            # Daily rates\n",
    "            (r'(?:(\\$|£|€)?(\\d[\\d,.]*)(k?)\\s*(?:to|-)\\s*)?(\\$|£|€)?(\\d[\\d,.]*)(k?)\\s*(?:p\\.d\\.|per\\s+day)\\b',\n",
    "             lambda m: ((parse_value(m.group(2), bool(m.group(3))) + parse_value(m.group(5), bool(m.group(6))))/2)\n",
    "                       if m.group(2) else parse_value(m.group(5), bool(m.group(6))), 'day'),\n",
    "\n",
    "            # Weekly rates\n",
    "            (r'(?:(\\$|£|€)?(\\d[\\d,.]*)(k?)\\s*(?:to|-)\\s*)?(\\$|£|€)?(\\d[\\d,.]*)(k?)\\s*(?:p\\.w\\.|per\\s+week)\\b',\n",
    "             lambda m: ((parse_value(m.group(2), bool(m.group(3))) + parse_value(m.group(5), bool(m.group(6))))/2)\n",
    "                       if m.group(2) else parse_value(m.group(5), bool(m.group(6))), 'week'),\n",
    "\n",
    "            # Monthly rates\n",
    "            (r'(?:(\\$|£|€)?(\\d[\\d,.]*)(k?)\\s*(?:to|-)\\s*)?(\\$|£|€)?(\\d[\\d,.]*)(k?)\\s*(?:p\\.m\\.|per\\s+month)\\b',\n",
    "             lambda m: ((parse_value(m.group(2), bool(m.group(3))) + parse_value(m.group(5), bool(m.group(6))))/2)\n",
    "                       if m.group(2) else parse_value(m.group(5), bool(m.group(6))), 'month'),\n",
    "\n",
    "            # Annual rates\n",
    "            (r'(?:(\\$|£|€)?(\\d[\\d,.]*)(k?)\\s*(?:to|-)\\s*)?(\\$|£|€)?(\\d[\\d,.]*)(k?)\\s*(?:p\\.a\\.|per\\s+year|per\\s+annum)\\b',\n",
    "             lambda m: ((parse_value(m.group(2), bool(m.group(3))) + parse_value(m.group(5), bool(m.group(6))))/2)\n",
    "                       if m.group(2) else parse_value(m.group(5), bool(m.group(6))), 'year'),\n",
    "\n",
    "            # Generic ranges\n",
    "            (r'(\\$|£|€)?(\\d[\\d,.]*)(k?)\\s*(?:to|-|−)\\s*(\\$|£|€)?(\\d[\\d,.]*)(k?)(?:\\s*(?:per|p\\.)(?:\\s*\\w+)?)?',\n",
    "             lambda m: (parse_value(m.group(2), bool(m.group(3))) + parse_value(m.group(5), bool(m.group(6))))/2, None),\n",
    "\n",
    "            # Single values with units\n",
    "            (r'(\\$|£|€)?(\\d[\\d,.]*)(k?)\\s*(?:p\\.h\\.|per\\s+hour|/hr?)\\b', lambda m: parse_value(m.group(2), bool(m.group(3))), 'hour'),\n",
    "            (r'(\\$|£|€)?(\\d[\\d,.]*)(k?)\\s*(?:p\\.d\\.|per\\s+day)\\b', lambda m: parse_value(m.group(2), bool(m.group(3))), 'day'),\n",
    "            (r'(\\$|£|€)?(\\d[\\d,.]*)(k?)\\s*(?:p\\.w\\.|per\\s+week)\\b', lambda m: parse_value(m.group(2), bool(m.group(3))), 'week'),\n",
    "            (r'(\\$|£|€)?(\\d[\\d,.]*)(k?)\\s*(?:p\\.m\\.|per\\s+month)\\b', lambda m: parse_value(m.group(2), bool(m.group(3))), 'month'),\n",
    "            (r'(\\$|£|€)?(\\d[\\d,.]*)(k?)\\s*(?:p\\.a\\.|per\\s+year|per\\s+annum)\\b', lambda m: parse_value(m.group(2), bool(m.group(3))), 'year'),\n",
    "\n",
    "            # Single values without units\n",
    "            (r'(\\$|£|€)?(\\d[\\d,.]*)(k?)\\b', lambda m: parse_value(m.group(2), bool(m.group(3))), None),\n",
    "        ]\n",
    "\n",
    "        for pattern, value_func, unit in patterns:\n",
    "            match = re.search(pattern, text, re.IGNORECASE)\n",
    "            if match:\n",
    "                try:\n",
    "                    value = value_func(match)\n",
    "                    # Default unit logic\n",
    "                    if unit is None:\n",
    "                        unit = 'year' if (value > 10000 or 'k' in match.group(0).lower()) else None\n",
    "                    return (round(value, 2), unit) if unit else (round(value, 2), 'year')\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "    return (None, None)\n",
    "\n",
    "job_ad_extend_coord_df[[\"salary_value\", \"salary_unit\"]] = job_ad_extend_coord_df[\"additionalSalaryText\"].apply(extract_salary_info).apply(pd.Series)\n",
    "job_ad_extend_coord_df[\"salary_unit\"] = job_ad_extend_coord_df[\"salary_unit\"].astype(str)\n"
   ],
   "id": "709973c9bc1c6201",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T03:40:21.390563Z",
     "start_time": "2025-03-17T03:40:21.377111Z"
    }
   },
   "cell_type": "code",
   "source": "job_ad_extend_coord_df.columns",
   "id": "75fc13948d48e33e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'title', 'abstract', 'content', 'additionalSalaryText',\n",
       "       'classification', 'subClassification', 'workType', 'area_location',\n",
       "       'address', 'Latitude', 'Longitude', 'global', 'state', 'country',\n",
       "       'cleaned_content', 'abstract_content', 'cleaned_title', 'is_english',\n",
       "       'salary_value', 'salary_unit'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T03:49:24.717100Z",
     "start_time": "2025-03-17T03:49:22.528379Z"
    }
   },
   "cell_type": "code",
   "source": [
    "job_ad_extend_coord_df = job_ad_extend_coord_df[['id', 'classification', 'subClassification', 'workType', 'Latitude', 'Longitude', 'global', 'state', 'country', 'abstract_content', 'cleaned_title', 'salary_value', 'salary_unit']]\n",
    "job_ad_extend_coord_df[\"id\"] = job_ad_extend_coord_df[\"id\"].astype(str)\n",
    "job_ad_extend_coord_df.to_csv(\"data/job_ad_extend_coord_df.csv\")"
   ],
   "id": "5ab27a6c2a39e947",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "5b39c6661064154e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Event Data",
   "id": "b6f51a64bf348373"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T05:06:33.910989Z",
     "start_time": "2025-03-17T05:06:32.117836Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Path to your .csv.xz file\n",
    "event_file_path = 'data/ads-50k-events.csv.gz'\n",
    "\n",
    "# Read the compressed CSV file into a DataFrame\n",
    "with lzma.open(event_file_path, 'rt', encoding='utf-8') as file:\n",
    "    event_data_df = pd.read_csv(event_file_path)"
   ],
   "id": "8627854e4c353676",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T05:07:55.569741Z",
     "start_time": "2025-03-17T05:07:54.124588Z"
    }
   },
   "cell_type": "code",
   "source": [
    "event_data_df['resume_id'] = event_data_df['resume_id'].astype(str)\n",
    "event_data_df['job_id'] = event_data_df['job_id'].astype(str)"
   ],
   "id": "cd48dcb4d8bf1e29",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T05:08:01.042812Z",
     "start_time": "2025-03-17T05:07:59.366340Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# remove duplicated rows\n",
    "event_data_dedup_df = event_data_df.groupby([\"event_datetime\",\"resume_id\",\"job_id\",\"event_platform\",\"kind\"]).first().reset_index()"
   ],
   "id": "cefae9bbdcf62fda",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T05:09:58.746472Z",
     "start_time": "2025-03-17T05:09:57.158051Z"
    }
   },
   "cell_type": "code",
   "source": "event_data_dedup_df.to_csv(\"event_data_dedup_df.csv\")",
   "id": "18398acfbabb0593",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Merge\n",
   "id": "73ce457710922603"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T06:18:40.843232Z",
     "start_time": "2025-03-17T06:18:03.920313Z"
    }
   },
   "cell_type": "code",
   "source": [
    "job_ad_extend_coord_df = pd.read_csv(\"data/job_ad_extend_coord_df.csv\",index_col=0)\n",
    "event_data_dedup_df = pd.read_csv(\"data/event_data_dedup_df.csv\",index_col=0)\n",
    "job_event_join_df = event_data_dedup_df.merge(job_ad_extend_coord_df, left_on='job_id', right_on='id', how='inner')\n",
    "job_event_join_df = job_event_join_df[['resume_id', 'job_id', 'event_platform',\n",
    "       'kind', 'cleaned_title', 'classification', 'subClassification',\n",
    "       'workType', 'Latitude', 'Longitude','global', 'state', 'country',\n",
    "       'abstract_content', 'salary_value','salary_unit']]\n",
    "job_event_join_df[\"resume_id\"] = job_event_join_df[\"resume_id\"].astype('str')\n",
    "job_event_join_df[\"job_id\"] = job_event_join_df[\"job_id\"].astype('str')\n",
    "job_event_join_df['cleaned_title'].fillna('NA',inplace = True)\n",
    "job_event_join_df['abstract_content'].fillna('NA',inplace = True)\n",
    "job_event_join_df[\"salary_unit\"].fillna('NA',inplace = True)\n",
    "job_event_join_df[\"salary_value\"].fillna(0,inplace = True)\n",
    "job_event_join_df.to_csv(\"data/job_event_join_df.csv\")"
   ],
   "id": "335150d2bdcbeeb1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\annab\\AppData\\Local\\Temp\\ipykernel_14620\\647220783.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  job_event_join_df['cleaned_title'].fillna('NA',inplace = True)\n",
      "C:\\Users\\annab\\AppData\\Local\\Temp\\ipykernel_14620\\647220783.py:11: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  job_event_join_df['abstract_content'].fillna('NA',inplace = True)\n",
      "C:\\Users\\annab\\AppData\\Local\\Temp\\ipykernel_14620\\647220783.py:12: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  job_event_join_df[\"salary_unit\"].fillna('NA',inplace = True)\n",
      "C:\\Users\\annab\\AppData\\Local\\Temp\\ipykernel_14620\\647220783.py:13: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  job_event_join_df[\"salary_value\"].fillna(0,inplace = True)\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-17T09:08:14.956902Z",
     "start_time": "2025-03-17T09:08:14.890843Z"
    }
   },
   "cell_type": "code",
   "source": "job_event_join_df.columns",
   "id": "d3efbe792de08dd9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['resume_id', 'job_id', 'event_platform', 'kind', 'cleaned_title',\n",
       "       'classification', 'subClassification', 'workType', 'Latitude',\n",
       "       'Longitude', 'global', 'state', 'country', 'abstract_content',\n",
       "       'salary_value', 'salary_unit'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
